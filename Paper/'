\section{Experiment Result}
In this section, we will analyze the performance of our algorithm compared to other SVD implementations on CPU and GPU.
In addition, we will discuss the GPU kernel profiling result to show how to improve our implementations.
We also show our performance on huge matrix size.

We tested our algorithm on an Intel Xeon Quad Core CPU 2.53GHz with a NVIDIA GeForce 750 Ti graphics processor 1.26GHz, and NVIDIA Tesla K40c processor 0.88GHz.
NVIDIA GeForce 750Ti is an Maxwell processor card has 640 CUDA Cores and 1 GB memory.
The memory bandwidth is 86.4 GB/s and the peak single precision floating point performance is 1.3 Tflops.
NVIDIA Tesla K40c processor has a Kepler GPU with 2880 CUDA cores and 12 GB memory.
The memory bandwidth is 288 GB/s and the peak single precision floating point performance is 4.29 Tflops.

\subsection{Comparision to other implementations}
We generated random bidiagonal matrices with double precision numbers in the range of 0 to 1.
In order to obtain relative accuracy experimental results, we generates 10 random matrix.
For each matrix, the SVD algorithm was executed 10 times on GPU.
The average performance does not vary much when more matrices and more times are used.

We compare our algorithm with CULA GPU library, Intel MKL library, Sheetal's QR implementation on S1070, and Liu's divide-and-conquer implementation on M2070.
Until now, CULA library only has QR routine culaDbdsqr.
Intel MKL library has both divide-and-conquer DBDSDC and QR routine DBDSQR.
Usually, divide-and-conquer routine is faster than QR routine, so we select DBDSDC instead of DBDSQR.
We measure the performance of CULA on Tesla K40c, and that of Intel MKL on an 8-core cpu with 16 threads.
For Sheetal's implementation, we use the experimental results of diagonalization listed in the table of papers.
The results of Liu's are estimated comprehensively according to their figures.

\begin{figure}[hbpt]
\centering
\includegraphics[width=0.5\textwidth]{svd_speedup}
\caption{Overall Performance Comparision}
\label{fig:svd_speedup}
\end{figure}
Figure \ref{fig:svd_speedup} compares the performance of our implementation on Tesla K40c GPU to other existing libraries and implementations.
We selects CULA QR routine BDBSQR as a baseline.
From the figure, we achieve a speedup of 3.8 to 36 over CULA culaDbdsqr routine on the same GPU,
while Intel MKL DBDSDC routine has a 2.9 to 4.3 speedup, and Liu's implementation has only 0.5 to 4.7 speedup over CULA library.
SHeetal's implementation is about 3 to 5.3 times slower than CULA library.

From the figure, we achieve a speedup of 1.3 to 8.3 over the Intel MKL Divide-Conquer Implementation on CPU, 4 to 7.2 over the Liu's Divide-conquer method, 15 to 288 over the QR implementation Sheetal Paper, and 

\subsection{Profiling Data}

\subsubsection{the Optimal Block Size}
In our design, the number of block size separated by the same length determines the execution time of the whole singular values.
An inappropriate block division will affect the performance on GPU heavily.

\begin{figure}[hbpt]
\centering
\includegraphics[width=0.5\textwidth]{length_block_num}
\caption{The optimal block number of different matrix size with double precision on Tesla K40c}
\label{fig:length_block_num}
\end{figure}
Figure \ref{fig:length_block_num} shows the elasped time of obtaining all the singular values with different number of blocks and different matrix size on GPU Tesla K40c with double precision.
The blue wavy curves show the relationship between the execution time and the number of blocks on the matrix size in the right column.
The left point of wave curve shows the minimal number of blocks should be allocated when the matrix size is identified.
In other words, the number of blocks should not be less than the left point on the wavy curves.
When matrix size becomes larger, the left point in wave curve trends to larger number of blocks.
The red circle curve in the figure shows the optimal size of blocks with minimal execution time on different size.

From Figure \ref{fig:length_block_num}, we can see that the optimal number of blocks increases when matrix size is less than $3k$,
keep stable when matrix size is in the range of $3k$ to $9k$,
and becomes the minimal number of blocks when matrix size is lager than $9K$.
Actually, when the block number is close to the optimal number, the execution time does not change too much.
Thus, it is not necessary to select the exact optimal block number.
But it is better to select the number of blocks around the optimal one.
Figure \ref{fig:length_block_single} is another test results of GeForce 750 Ti GPU.
The curves are very similar with the curves in Figure \ref{fig:length_block_num}.

\begin{figure}[hbpt]
\centering
\includegraphics[width=0.5\textwidth]{length_block_single}
\caption{The optimal block number of different matrix size with single precision on GeForce 750 Ti}
\label{fig:length_block_single}
\end{figure}
%The reason of the optimal number of blocks is determined by the CUDA cores.
%The Tesla K40c has 15 multiprocessors (MP) and 192 CUDA cores per MP.
%Thus, the total CUDA cores are $2880$ in Tesla K40.
%The GPU code is actually executed in groups of 32 threads called a warp concurrently.

%The optimal GPU block size is not the less, the better.
%It depends on the number of singular values in the subintervals.
%If one have more, GPU will have to wait.
%If all subintervals have the number of singular deviate from the integer multiples of a warp. The GPU efficiency will be less.


\subsubsection{Comparasion of two different Singular Value Design}
In this part, we compare the execution time on two different singular value kernels.

\begin{figure}[hbpt]
\centering
\includegraphics[width=0.5\textwidth]{compare_value_kernel}
\caption{The optimal block number of different matrix size with single precision on GeForce 750 Ti}
\label{fig:compare_value_kernel}
\end{figure}
Figure \ref{fig:compare_value_kernel} shows the execution time of obtaining all singular values on two different GPU implementations.
From the figure, we can see that when matrix size is less than $9k$, length division kernel run a little faster than number division kernel.
When matrix size is larger than $9k$, the execution time of length division kernel rises quickly, while the execution time of number division kernel only rises linearly.
Thus, when matrix size becomes larger than $9k$, it is better to select number division kernel.

\textbf{I also want to check the reason why the execution time of number division version is larger than that of length division when matrix is less than $9k$. Thus the figure may be re-draw.}

\subsubsection{Tolerance in Bisection Algorithm}
Since the bisection algorithm is an approximate algorithm to calculate the singular values, we should test the effect of different error tolerance.
The error tolerance $err$ means that the error between the singular values of our algorithm and the actual singular values are less than $err$.
It determines the accuracy of singular value and therefore the orthogonality of singular vectors.
As we know, the more accuracy of singular values are, the more execution time should be spent.
However, it is important to know the incremental execution time to determine which error tolerance is suitable for different applications.
We test our algorithm on different error tolerance.
The error tolerance is between $10^{-5}$ to $10^{-16}$ with tenfolder decreasing.

\textbf{I'm still thinking how to draw this figure.}
\begin{figure}[hbpt]
\centering
\includegraphics[width=0.5\textwidth]{tolerance}
\caption{Average Extra Execution Time When the accuracy increase Performance Comparision}
\label{fig:tolerance}
\end{figure}
Figure \ref{fig:tolerance} shows the average increased execution time when the accuracy of singular values goes up a higher level on different matrix size.
In other word, it shows the average increased execution time when the error tolerance becomes smaller from $10^{-x}$ to $10^{-(x+1)}$.
From the figure, we can see that when matrix size is smaller than 12000, the additional execution time is only less than $20 ms$ when the error tolerance rises a level.
When the matrix size is larger than 15000, the additional execution time is a little higher about $40 ms$ per level.


\subsection{Huge Size Result}
\textbf{A table to shows the Huge Size Result. This part do not compare to other results.} 
